## **背景**
- 预定义格式受限
  - 受限的行动空间：预定义工具 
  - 有限的灵活性：无法组合多个工具 
## **动机**
- Python代码（多轮交互）
  - 代码行动：本质支持控制与数据流程 
  - 动态修正先前行动 
  - 基于新观察发出新行动 
## **实现**
- CodeActAgent
  - 模型：Llama2 7B、Mistral 7B
  - 功能：生成可执行Python代码作为行动 
## **CodeActInstruct指令微调数据集（Section 3.1）**
- 4个交互场景
  - 信息搜索 
  - 软件包（工具）使用[含APPS代码生成、MATH] 
  - 外部存储：SQL - based、Pandas - based 
  - 机器人规划 
- 下采样
  - 仅保留具挑战性实例 
    - 轨迹生成更经济高效 
    - 删除简单实例 
- 交互验证转变
  - 从单轮交互后验证变为多轮交互 
  - 代码生成时，提供上下文示例（测试用例） 
- 轨迹生成（子采样）
  - 除代码生成外：用gpt - 3.5 - turbo、claude1 & claude2 
  - 代码生成：先gpt - 3.5 - turbo，不行再gpt - 4 
- Agent改进能力增强
  - 如错误代码调试 
  - 强化“试错 - 反思 - 改进”逻辑 
  - 选高质量子集，含遇错误、交互中自我纠正的轨迹样本 
## **CodeActAgent训练与评估（Section 3.2）**
- 训练
  - 基于3.1节微调2个7B模型 
  - 训练评估配置：Llama2为4096 tokens，Mistral为16384 tokens 
- 结论
  - CodeAct表现出色，完胜开源，逼近闭源gpt - 4 
  - 未针对文本优化，但可比肩专门调优的7B Agent 
  - 对一般LLM任务性能保持/提升 
  - 消融实验证明CodeActInstruct和general conversations对保持通用任务能力的重要性 
  - 多维度验证
    - 交互k = 5的成功率 
    - 域外Agent任务→不同格式泛化性 
    - 通用LLM评估任务→通用能力 