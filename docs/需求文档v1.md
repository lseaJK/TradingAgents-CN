## 大模型调用前 Token 数判断需求

在每次调用大模型（尤其是 deep_think_llm 方法）前，需要先获取输入的 token 数。
如果原始 input 的 token 数超过模型的限制，则本次请求无法得到结果。
希望能在调用 deep_think_llm 之前提前判断和处理这种情况，而不仅仅依赖 _generate 方法返回的 input_token 长度。

### 优化方案
1. 优先使用 `len(prompt)` 判断输入字符长度，如果明显未超限，则直接使用原始输入，提升速度。
2. 只有当字符长度接近或超过最大输入字符数的 80% 时，再用本地分词器精确计算 token 数。
3. 如果 token 数超过最大输入 token 数的 80%，则只保留前 80% 的 token，并在结尾加上 '...'，以保证有足够空间用于模型输出。
4. 这样可以兼顾性能和准确性，避免无输出空间或报错。
